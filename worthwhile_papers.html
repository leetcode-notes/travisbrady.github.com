<html>
    <head>
        <title>Worthwhile Papers</title>
        <style>
            body { font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
                color: #3B3B3B
            }
            ul { list-style-type: none }
            a {
                color: black
            }
            a:hover { color: #9370DB }
            p { font: italic 0.5em Georgia, serif;}
            .inner { 
              font: italic 0.5em Georgia, serif;
            }
            .container {
              width: 80%
            }
            blockquote {
              font-style: normal;
              border-left: 4px solid #CCC;
              padding-left: 8px;
            }
            .big_link {
              text-decoration: none
            }
        </style>
    </head>
    <body>
      <div class="container">
        <h1 style="font-size: 4em">Worthwhile Papers<h1>
        <p>Articles worth your time if you work on production machine learning systems</p>
        <ul>
            <li><a class="big_link" href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf">Ad Click Prediction: a View from the Trenches</a>
              <p>A well-known paper from a large group a Google sharing details on Google's sponsored search system. I like this paper because it naturally mixes lots of
              practical lessons about using ML in production with a notable theory contribution (FTRL). It's a bit dated now I suppose but definitely worth a read at least in
              part because it is also (to my knowledge) <a href="https://research.google.com/pubs/author38217.html">D Sculley's</a> opening salvo in his series of articles
              about using ML in production</p>
            </li>
            <li>
              <a class="big_link" href="https://research.google.com/pubs/pub43146.html">Machine Learning: The High Interest Credit Card of Technical Debt</a>
              <p>Another excellent paper from Google (including D Sculley) with a very practical bent. This NIPS 2014 paper covers a few of the arguably most important ways in
              which you can shoot yourself in the foot putting machine learning in production.
              </p>
            </li>
            <li><a class="big_link" href="http://www.kdd.org/kdd2017/papers/view/an-efficient-bandit-algorithm-for-realtime-multivariate-optimization">An efficient bandit algorithm for realtime multivariate optimization</a>
              <p>KDD '17 paper from Amazon with a very impressive result in a production e-commerce system. If you've worked on these kinds of problems before the 21% improvement
              they claim is almost too good to be true. From a product perspective this paper is about laying out a small widget or portion of a page. Typically these kinds of things are handled solely by product managers who come up with few variations which are then tested in a standard controlled experiment. This is sub-optimal because you often can't test every possible combination and so intuition rules what ultimately gets tested.</p>
              <blockquote>
                <p>After only a single week of online optimization, we saw a 21% conversion increase compared to the median layout. Our technique is currently being deployed to optimize content across several locations at Amazon.com</p></blockquote>
            </li>
            <li>
              <a class="big_link" href="http://additivegroves.net/papers/a9ext_sigir16.pdf">Amazon Search: The Joy of Ranking Products</a>(<a href="https://www.youtube.com/watch?v=NLrhmn-EZ88">video</a>)
              <p>A short but interesting read for anyone working on e-commerce product ranking. There is nothing particularly groundbreaking here but a few details stand out
              for practitioners:
              <ul class="inner">
                <li>- They rank using lambdamart models trained in part on click feedback</li>
                <li>- They randomly sample things never rendered as part of the negative class</li>
                <li>- They train >100 models corresponding to (site, category) pairs (eg amazon_japan, electronics)</li>
                <li>- Model complexity somewhat modest at ~200 trees per model</li>
                <li>- Their target is a mixture of clicks, cart adds and purchases</li>
                <li>- They use custom feature evaluation code written by Sorokina (<a href="https://github.com/dariasor/TreeExtra">github:dariasor/TreeExtra</a>)
              </ul>
              </p>
            </li>
            <li>
              <a class="big_link" href="https://arxiv.org/abs/1701.04099">Field-aware Factorization Machines in a Real-world Online Advertising System</a>
              <p>Criteo team discusses challenges encountered when they tried to take a Kaggle-winning CTR estimation model (based on Field Aware Factorization Machines) into 
              "real" production to replace a status quo logistic regression-based model. Unsurprisingly there were challenges. In particular, while offline evaluation results
              appeared better than the status quo, the FFM models took substantially longer to train. From there they go on to describe distributed training of FFMs.
              </p>
            </li>
        </ul>
        <a class="big_link" href="index.html">Home</a>
    </div>
<script type="text/javascript">
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
    try {
        var pageTracker = _gat._getTracker("UA-1125655-4");
        pageTracker._trackPageview();
        } catch(err) {}</script>
</script>


